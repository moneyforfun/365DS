{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":30698,"isInternetEnabled":true,"language":"python","sourceType":"script","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# %% [code]\n# %% [code]\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-04-25T15:30:41.903888Z\",\"iopub.execute_input\":\"2024-04-25T15:30:41.904570Z\",\"iopub.status.idle\":\"2024-04-25T15:30:42.802063Z\",\"shell.execute_reply.started\":\"2024-04-25T15:30:41.904538Z\",\"shell.execute_reply\":\"2024-04-25T15:30:42.801285Z\"}}\n# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-04-25T15:30:42.803592Z\",\"iopub.execute_input\":\"2024-04-25T15:30:42.803949Z\",\"iopub.status.idle\":\"2024-04-25T15:30:42.807920Z\",\"shell.execute_reply.started\":\"2024-04-25T15:30:42.803925Z\",\"shell.execute_reply\":\"2024-04-25T15:30:42.807044Z\"}}\n#this is the condensed version of the model from the 365DS_ML file\n\n# %% [code] {\"execution\":{\"iopub.status.busy\":\"2024-04-26T14:22:30.431508Z\",\"iopub.execute_input\":\"2024-04-26T14:22:30.432056Z\",\"iopub.status.idle\":\"2024-04-26T14:22:30.455947Z\",\"shell.execute_reply.started\":\"2024-04-26T14:22:30.432024Z\",\"shell.execute_reply\":\"2024-04-26T14:22:30.454892Z\"}}\n#importing libraries\nimport numpy as np\nimport pandas as pd \nimport pickle\nfrom sklearn.preprocessing import StandardScaler\nfrom sklearn.base import BaseEstimator,TransformerMixin\n\n#introducing classes\nclass CustomScaler(BaseEstimator,TransformerMixin):\n    \n    def __init__(self,columns):\n        self.scaler = StandardScaler()\n        self.columns = columns\n        self.mean_ = None\n        self.var_ = None\n        \n    \n    def fit(self, X, y=None):\n        self.scaler = StandardScaler()\n        self.scaler.fit(X[self.columns], y)\n        self.mean_ = np.mean(X[self.columns])\n        self.var_ = np.var(X[self.columns])\n        return self\n    \n\n    def transform(self, X, y=None, copy=None):\n        init_col_order = X.columns\n        X_scaled = pd.DataFrame(self.scaler.transform(X[self.columns]), columns=self.columns)\n        X_not_scaled = X.loc[:,~X.columns.isin(self.columns)]\n        return pd.concat([X_not_scaled, X_scaled], axis=1)[init_col_order]\n\n#creating a class that will be used for new data prediction\n\nclass abs_model():\n\n    def __init__(self,model_file,scaler_file):\n        #reading in the model and scaler files- files that were saved\n        with open('mlmodel','rb') as model_file, open('abscaler','rb') as scaler_file:\n            self.reg = pickle.load(model_file)\n            self.scaler = pickle.load(scaler_file)\n            self.data = None\n    #preprocessing data files-as previously\n    def load_and_clean_data(self, data_file):\n        \n        df = pd.read_csv('/kaggle/input/df-prep-fixed/df_prep (1).csv',encoding='utf-8')\n        #storing data in new variable\n        self.df_with_predictions = df.copy()\n        df = df.drop(['Unnamed: 0'],axis=1)\n        df = df.drop(['ID'],axis=1)\n        #preserving code created in the previous section- adding new column with NaN strings \n        df['Absenteeism Time in Hours'] = 'NaN'\n        \n        #creating separate df with dummy values of all variables\n        reason_columns = pd.get_dummies(df['Reason for Absence'],drop_first=True)\n        reason_type_1 = reason_columns.loc[:,1:14].max(axis=1)\n        reason_type_2 = reason_columns.loc[:,15:17].max(axis=1)\n        reason_type_3 = reason_columns.loc[:,18:21].max(axis=1)\n        reason_type_4 = reason_columns.loc[:,22:28].max(axis=1)\n        \n        df = df.drop(['Reason for Absence'],axis=1)\n        \n        df = pd.concat([df, reason_type_1,reason_type_2,reason_type_3,reason_type_4], axis =1 )\n        column_names = ['Date', 'Transportation Expense', 'Distance to Work', 'Age',\n       'Daily Work Load Average', 'Body Mass Index', 'Education',\n       'Children', 'Pets', 'Absenteeism Time in Hours', 'Reason_1', 'Reason_2', 'Reason_3','Reason_4']\n        df.columns = column_names\n        column_names_reordered = ['Reason_1', 'Reason_2', 'Reason_3','Reason_4','Date', 'Transportation Expense', 'Distance to Work', 'Age',\n       'Daily Work Load Average', 'Body Mass Index', 'Education',\n       'Children', 'Pets', 'Absenteeism Time in Hours']\n\n        df = df[column_names_reordered]\n        df['Date'] = pd.to_datetime(df['Date'],format= '%d/%m/%Y')\n        list_mo = []\n        for i in range(dfmod.shape[0]):\n            list_mo.append(df['Date'][i].month)\n        df['Month'] = list_mo\n        list_day = []\n        list_year = []\n\n        for i in range(dfmod.shape[0]):\n            list_day.append(df['Date'][i].weekday())\n            list_year.append(df['Date'][i].year)\n\n        df['WeekDay'] = list_day\n        df['Year'] = list_year\n        df['Education'] = dfmod['Education'].map({1:0, 2:1, 3:1,4:1})\n        \n        df = df.fillna(value=0)\n    \n        df = df.drop(['Date'],axis=1)\n        df = df.drop(['Absenteeism Time in Hours','WeekDay','Daily Work Load Average','Distance to Work'],axis=1)\n        \n        self.preprocessed_data = df.copy()\n        \n        self.data = self.scaler.transform(df)\n        \n    def predicted_probability(self):\n        if (self.data is not None):\n            pred = self.reg.predict_proba(self.data)[:.1]\n            return pred\n    def predicted_output_category(self):\n        if(self.data is not None):\n            pred_outputs = self.reg.predict(self.data)\n            return pred_outputs\n        \n    def predicted_outputs(self):\n        if (self.data is not None):\n            self.preprocessed_data['Probability'] = self.reg.predict_proba(self.data)[:.1]\n            self.preprocessed_data['Prediction'] = self.reg.predict(self.data)\n            return self.preprocessed_data","metadata":{"_uuid":"98cf66f8-3f1e-4553-a49d-27d5fcf2d272","_cell_guid":"49d6f903-04aa-4944-be93-e838fe2a4f52","collapsed":false,"jupyter":{"outputs_hidden":false},"trusted":true},"execution_count":null,"outputs":[]}]}